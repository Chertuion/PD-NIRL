{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, r2_score, mean_squared_error\n",
    "import seaborn as sns\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "import random\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed) \n",
    "    torch.cuda.manual_seed_all(seed) \n",
    "\n",
    "    torch.backends.cudnn.deterministic = True  \n",
    "    torch.backends.cudnn.benchmark = False \n",
    "\n",
    "seed = 1\n",
    "set_seed(seed)\n",
    "\n",
    "csv_path = '/data/home/wxl22/Classification_of_Varieties/process_utils/processed_spectra.csv' \n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "\n",
    "spectra_data = df['data'].apply(eval).to_numpy()  \n",
    "labels = df['label'].to_numpy()  \n",
    "\n",
    "\n",
    "X = np.array(list(spectra_data)) \n",
    "y = labels\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)  \n",
    "\n",
    "\n",
    "label_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
    "print(\"Label Mapping: \", label_mapping)\n",
    "print(\"Number of classes:\", len(label_mapping))\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y_encoded, test_size=0.2, random_state=seed, stratify=y_encoded\n",
    ")\n",
    "\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "y_test = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "\n",
    "class SpectraDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "\n",
    "train_dataset = SpectraDataset(X_train, y_train)\n",
    "test_dataset = SpectraDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "\n",
    "class VectorQuantizer(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim, commitment_cost, init_embeddings=None):\n",
    "        super(VectorQuantizer, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.commitment_cost = commitment_cost\n",
    "\n",
    "        self.embeddings = nn.Embedding(self.num_embeddings, self.embedding_dim)\n",
    "        if init_embeddings is not None:\n",
    "            self.embeddings.weight.data.copy_(torch.from_numpy(init_embeddings))\n",
    "        else:\n",
    "            self.embeddings.weight.data.uniform_(-1/self.num_embeddings, 1/self.num_embeddings)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "\n",
    "        inputs = inputs.permute(0, 2, 1).contiguous()  \n",
    "        input_shape = inputs.shape\n",
    "        flat_input = inputs.view(-1, self.embedding_dim)  \n",
    "\n",
    "        distances = (torch.sum(flat_input**2, dim=1, keepdim=True)\n",
    "                     + torch.sum(self.embeddings.weight**2, dim=1)\n",
    "                     - 2 * torch.matmul(flat_input, self.embeddings.weight.t()))  \n",
    "\n",
    "\n",
    "        encoding_indices = torch.argmin(distances, dim=1).unsqueeze(1) \n",
    "        encodings = torch.zeros(encoding_indices.size(0), self.num_embeddings, device=inputs.device)\n",
    "        encodings.scatter_(1, encoding_indices, 1) \n",
    "\n",
    "    \n",
    "        quantized = torch.matmul(encodings, self.embeddings.weight).view(input_shape) \n",
    "\n",
    "\n",
    "        e_latent_loss = F.mse_loss(quantized.detach(), inputs)\n",
    "        q_latent_loss = F.mse_loss(quantized, inputs.detach())\n",
    "        loss = q_latent_loss + self.commitment_cost * e_latent_loss\n",
    "\n",
    "        quantized = inputs + (quantized - inputs).detach()\n",
    "\n",
    "        quantized = quantized.permute(0, 2, 1).contiguous()  \n",
    "\n",
    "        return quantized, loss\n",
    "\n",
    "\n",
    "class ImportanceSplitNet(nn.Module):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv_multi = nn.Conv1d(embedding_dim * 6, embedding_dim * 2, kernel_size=3, padding=1)\n",
    "        \n",
    "\n",
    "        self.gate_fc = nn.Linear(embedding_dim * 2, embedding_dim * 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (B, 6*embedding_dim, L)\n",
    "           —— 假设在外部已经将三路输出拼接得到 embedding_dim*6 个通道\n",
    "        \"\"\"\n",
    "\n",
    "        out = self.conv_multi(x)\n",
    "        \n",
    "        gating_input = F.adaptive_avg_pool1d(out, output_size=1).squeeze(-1)\n",
    "\n",
    "        gating_score = torch.sigmoid(self.gate_fc(gating_input))\n",
    "\n",
    "        gating_score = gating_score.unsqueeze(-1) \n",
    "        gating_score = gating_score.expand(-1, -1, out.size(-1))\n",
    "        \n",
    "        weighted_out = out * gating_score \n",
    "        \n",
    "        ch_score = gating_score.mean(dim=-1)\n",
    "        \n",
    "        threshold = 0.5\n",
    "        ch_mask = (ch_score > threshold).float().unsqueeze(-1)  \n",
    "        ch_mask = ch_mask.expand(-1, -1, weighted_out.size(-1)) \n",
    "        \n",
    "\n",
    "        important_features   = weighted_out * ch_mask\n",
    "        unimportant_features = weighted_out * (1 - ch_mask)\n",
    "        \n",
    "        return important_features, unimportant_features\n",
    "\n",
    "class Orthogonal_Model(nn.Module):\n",
    "    def __init__(self, embedding_dim, mlp_in):\n",
    "        super(Orthogonal_Model, self).__init__()\n",
    "        self.invariant_axis = nn.Parameter(torch.empty(mlp_in, mlp_in), requires_grad=True)\n",
    "        self.relevant_axis = nn.Parameter(torch.empty(mlp_in, mlp_in), requires_grad=True)\n",
    "        nn.init.xavier_uniform_(self.invariant_axis, gain=nn.init.calculate_gain('relu'))\n",
    "        nn.init.xavier_uniform_(self.relevant_axis, gain=nn.init.calculate_gain('relu'))\n",
    "\n",
    "        self.conv_multi = nn.Conv1d(embedding_dim * 6, embedding_dim * 4, kernel_size=3, padding=1)\n",
    "\n",
    "\n",
    "        self.invariant_axis = nn.Parameter(torch.randn(4 * embedding_dim))\n",
    "        self.relevant_axis = nn.Parameter(torch.randn(4 * embedding_dim))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            self.invariant_axis /= torch.norm(self.invariant_axis) + 1e-10\n",
    "            self.relevant_axis -= torch.dot(self.relevant_axis, self.invariant_axis) * self.invariant_axis\n",
    "            self.relevant_axis /= torch.norm(self.relevant_axis) + 1e-10\n",
    "\n",
    "    def similarity(self, feature1, feature2, type='cos'):\n",
    "        sim = 0\n",
    "        if type == 'cos':\n",
    "            norm1 = torch.norm(feature1, dim=1, keepdim=True)\n",
    "            norm2 = torch.norm(feature2, dim=1, keepdim=True)\n",
    "            sim = feature1 * feature2 / (norm1 * norm2)\n",
    "            sim = torch.sum(sim, dim=1)\n",
    "            sim = torch.max(torch.abs(sim))\n",
    "        if type == 'pearson':\n",
    "            sim = []\n",
    "            for i in range(feature1.shape[0]):\n",
    "                sim_temp = np.corrcoef(feature1[i].cpu().detach().numpy(), feature2[i].cpu().detach().numpy())\n",
    "                # print(sim_temp)\n",
    "                sim.append(sim_temp[0][1])\n",
    "            sim = max(sim, key=abs)\n",
    "        return sim\n",
    "\n",
    "    def orthogonal_loss(self):\n",
    "            \n",
    "            invariant_axis = self.invariant_axis / (torch.norm(self.invariant_axis) + 1e-10)  # (C,)\n",
    "            relevant_axis = self.relevant_axis / (torch.norm(self.relevant_axis) + 1e-10)      # (C,)\n",
    "\n",
    "\n",
    "            o_loss = torch.dot(invariant_axis, relevant_axis)  # 标量\n",
    "\n",
    "            o_loss = o_loss ** 2\n",
    "            \n",
    "            return o_loss\n",
    "\n",
    "    def forward(self, x):\n",
    "            \n",
    "            out = self.conv_multi(x)  \n",
    "            B, C, L = out.shape \n",
    "            \n",
    "\n",
    "            invariant_axis = self.invariant_axis / (torch.norm(self.invariant_axis) ** 2 + 1e-10) \n",
    "            relevant_axis = self.relevant_axis / (torch.norm(self.relevant_axis) ** 2 + 1e-10)   \n",
    "            \n",
    "\n",
    "\n",
    "            invariant_dot = torch.matmul(out.transpose(1, 2), invariant_axis)  \n",
    "            relevant_dot = torch.matmul(out.transpose(1, 2), relevant_axis)    \n",
    "            \n",
    "   \n",
    "            invariant_dot = invariant_dot.unsqueeze(1) \n",
    "            relevant_dot = relevant_dot.unsqueeze(1)   \n",
    "            \n",
    "        \n",
    "            invariant_axis = invariant_axis.unsqueeze(0).unsqueeze(2)  \n",
    "            relevant_axis = relevant_axis.unsqueeze(0).unsqueeze(2)    \n",
    "            \n",
    "\n",
    "            invariant_features = invariant_dot * invariant_axis  \n",
    "            relevant_features = relevant_dot * relevant_axis      \n",
    "            \n",
    "\n",
    "            invariant_features = invariant_features[:, :C//2, :] \n",
    "            relevant_features = relevant_features[:, C//2:, :]   \n",
    "            \n",
    "            return invariant_features, relevant_features\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, n_layers, in_dim, hidden, out_dim, dropout):\n",
    "        super(MLP, self).__init__()\n",
    "        self.lins = nn.ModuleList()\n",
    "        self.lins.append(nn.Linear(in_dim, hidden))\n",
    "        for _ in range(n_layers - 2):\n",
    "            self.lins.append(nn.Linear(hidden, hidden))\n",
    "        self.lins.append(nn.Linear(hidden, out_dim))\n",
    "        self.dropout = dropout\n",
    "\n",
    "        for i in range(len(self.lins)):\n",
    "            nn.init.xavier_uniform_(self.lins[i].weight, gain=nn.init.calculate_gain('relu'))\n",
    "    def forward(self, features):\n",
    "        for i, lin in enumerate(self.lins[:-1]):\n",
    "            features = lin(features)\n",
    "            features = F.relu(features)\n",
    "            features = F.dropout(features, p=self.dropout, training=self.training)\n",
    "        features = self.lins[-1](features)\n",
    "        return features\n",
    "\n",
    "\n",
    "class SpectraCNNWithAttentionVQ(nn.Module):\n",
    "    def __init__(self, num_classes, num_embeddings=512, embedding_dim=128, commitment_cost=0.25, num_heads=4, init_embeddings=None):\n",
    "        super(SpectraCNNWithAttentionVQ, self).__init__()\n",
    "        dropout = 0.3\n",
    "        self.conv1 = nn.Conv1d(1, 64, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        self.conv2 = nn.Conv1d(64, embedding_dim, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm1d(embedding_dim)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        self.conv3 = nn.Conv1d(embedding_dim, embedding_dim * 2, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm1d(embedding_dim * 2)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "\n",
    "        self.pool3 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv4 = nn.Conv1d(embedding_dim * 2, embedding_dim * 2, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm1d(embedding_dim * 2)\n",
    "        self.pool4 = nn.MaxPool1d(kernel_size=2, stride=2) \n",
    "\n",
    "        self.conv5 = nn.Conv1d(embedding_dim * 2, embedding_dim * 2, kernel_size=5, padding=2)\n",
    "        self.bn5 = nn.BatchNorm1d(embedding_dim * 2)\n",
    "        self.pool5 = nn.AvgPool1d(kernel_size=2, stride=2) \n",
    "\n",
    "        self.conv6 = nn.Conv1d(embedding_dim * 2, embedding_dim * 2, kernel_size=7, padding=3)\n",
    "        self.bn6 = nn.BatchNorm1d(embedding_dim * 2)\n",
    "        self.pool6 = nn.MaxPool1d(kernel_size=2, stride=2) \n",
    "\n",
    "        self.conv_multi = nn.Conv1d(embedding_dim * 6, embedding_dim * 2, kernel_size=3, padding=1)  \n",
    "        self.orthogonal = Orthogonal_Model(embedding_dim, embedding_dim * 2)\n",
    "\n",
    "        self.residual_conv = nn.Conv1d(1, embedding_dim * 2, kernel_size=1)\n",
    "\n",
    "        self.vq = VectorQuantizer(num_embeddings, embedding_dim * 2, commitment_cost, init_embeddings)\n",
    "\n",
    "\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=embedding_dim * 2, num_heads=num_heads, batch_first=True)\n",
    "\n",
    "        self.gate = nn.Conv1d(embedding_dim * 2, embedding_dim * 2, kernel_size=1)\n",
    "\n",
    "        self.fc1 = nn.Linear(embedding_dim * 256, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "        self.mlp = MLP(3, embedding_dim * 256, 128, num_classes, dropout)\n",
    "\n",
    "        self.dropout4 = nn.Dropout(0.58)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = x.unsqueeze(1)  \n",
    "        \n",
    "        x = F.relu(self.bn1(self.conv1(x)))  \n",
    "        x = self.dropout1(x)\n",
    "       \n",
    "        x = F.relu(self.bn2(self.conv2(x))) \n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        x = F.relu(self.bn3(self.conv3(x))) \n",
    "        x = self.dropout3(x)\n",
    "\n",
    "        x1 = F.relu(self.bn4(self.conv4(x)))\n",
    "        x1 = self.pool4(x1)\n",
    "        x2 = F.relu(self.bn5(self.conv5(x)))\n",
    "        x2 = self.pool5(x2)\n",
    "        x3 = F.relu(self.bn6(self.conv6(x)))\n",
    "        x3 = self.pool6(x3)\n",
    "\n",
    "        xn = torch.cat([x1, x2, x3], dim=1)  \n",
    "\n",
    "        invariant, spurious = self.orthogonal(xn)\n",
    "\n",
    "        o_loss = self.orthogonal.orthogonal_loss()\n",
    "\n",
    "\n",
    "\n",
    "        # 向量量化\n",
    "        x = self.pool3(x)\n",
    "        x_vq, vq_loss = self.vq(x)  \n",
    "   \n",
    "        gate = torch.tanh(self.gate(x))  \n",
    "\n",
    "        x = gate * invariant + (1 - gate) * x_vq  \n",
    "\n",
    "    \n",
    "        x = x.permute(0, 2, 1).contiguous()  \n",
    "        attn_output, _ = self.attention(x, x, x)  \n",
    "        attn_output = attn_output.permute(0, 2, 1).contiguous()  \n",
    "        x = attn_output.view(x.size(0), -1) \n",
    "        x = F.relu(self.fc1(x)) \n",
    "        x = self.dropout4(x)\n",
    "       \n",
    "        logits = self.fc2(x) \n",
    "       \n",
    "        probabilities = F.softmax(logits, dim=1)  \n",
    "\n",
    "        invariant_logits = F.softmax(self.mlp(invariant.view(x.size(0), -1)), dim=1)\n",
    "\n",
    "\n",
    "\n",
    "        # print(\"o_loss:\", o_loss)\n",
    "        return logits, probabilities, vq_loss, o_loss, invariant_logits\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "num_classes = len(label_encoder.classes_)\n",
    "print(\"Number of classes:\", num_classes)\n",
    "\n",
    "\n",
    "num_classes = 80 \n",
    "num_embeddings = 128  \n",
    "embedding_dim = 64\n",
    "commitment_cost = 0.4 \n",
    "num_heads = 64 \n",
    "\n",
    "\n",
    "model = SpectraCNNWithAttentionVQ(num_classes=num_classes,\n",
    "                                    num_embeddings=num_embeddings,\n",
    "                                    embedding_dim=embedding_dim,\n",
    "                                    commitment_cost=commitment_cost,\n",
    "                                    num_heads=num_heads)\n",
    "print(model)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)  \n",
    "\n",
    "\n",
    "criterion_cls = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "max_train_accuracy = 0.0\n",
    "max_train_epoch = 0\n",
    "test_accuracy_at_max_train = 0.0\n",
    "test_loss_at_max_train = 0.0\n",
    "precision_at_max_train = 0.0\n",
    "recall_at_max_train = 0.0\n",
    "f1_at_max_train = 0.0\n",
    "\n",
    "\n",
    "best_test_labels = []\n",
    "best_test_predictions = []\n",
    "\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "r2_scores = []\n",
    "rmse_scores = []\n",
    "\n",
    "\n",
    "num_epochs = 500\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    for data, labels in train_loader:\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "\n",
    "\n",
    "        logits, probabilities, vqloss, o_loss, invariant_logits = model(data)\n",
    "        loss = criterion_cls(logits, labels) + criterion_cls(invariant_logits, labels) + vqloss + o_loss\n",
    "\n",
    "  \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(logits.data, 1)\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "    train_accuracy = correct_train / total_train\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    running_test_loss = 0.0\n",
    "    correct_test = 0\n",
    "    total_test = 0\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    predicted_classes = []\n",
    "    with torch.no_grad():\n",
    "        for data, labels in test_loader:\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            logits, probabilities, vqloss, o_loss, invariant_logits = model(data)\n",
    "            loss = criterion_cls(logits, labels) + criterion_cls(invariant_logits, labels) + vqloss + o_loss\n",
    "            running_test_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(logits.data, 1)\n",
    "            total_test += labels.size(0)\n",
    "            correct_test += (predicted == labels).sum().item()\n",
    "\n",
    "          \n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.extend(probabilities.cpu().numpy())\n",
    "            predicted_classes.extend(predicted.cpu().numpy())\n",
    "\n",
    "    avg_test_loss = running_test_loss / len(test_loader)\n",
    "    test_accuracy = correct_test / total_test\n",
    "\n",
    "\n",
    "    precision = precision_score(all_labels, predicted_classes, average='weighted', zero_division=0)\n",
    "    recall = recall_score(all_labels, predicted_classes, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(all_labels, predicted_classes, average='weighted', zero_division=0)\n",
    "\n",
    "\n",
    "    if train_accuracy > max_train_accuracy:\n",
    "        max_train_accuracy = train_accuracy\n",
    "        max_train_epoch = epoch\n",
    "        test_accuracy_at_max_train = test_accuracy\n",
    "        test_loss_at_max_train = avg_test_loss\n",
    "        precision_at_max_train = precision\n",
    "        recall_at_max_train = recall\n",
    "        f1_at_max_train = f1\n",
    "        best_test_labels = copy.deepcopy(all_labels)\n",
    "        best_test_predictions = copy.deepcopy(predicted_classes)\n",
    "\n",
    "\n",
    "\n",
    "    train_losses.append(avg_train_loss)\n",
    "    test_losses.append(avg_test_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    test_accuracies.append(test_accuracy)\n",
    "    precision_scores.append(precision)\n",
    "    recall_scores.append(recall)\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
    "          f\"Train Loss: {avg_train_loss:.4f} (Cls: {avg_train_loss:.4f}), \"\n",
    "          f\"Train Accuracy: {train_accuracy:.4f}, \"\n",
    "          f\"Test Loss: {avg_test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}, \"\n",
    "          f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1:.4f}\")\n",
    "\n",
    "\n",
    "cm_test = confusion_matrix(best_test_labels, best_test_predictions)\n",
    "print(len(best_test_predictions))\n",
    "\n",
    "print(\"\\n==========================\")\n",
    "print(f\"训练准确率最高的 Epoch: {max_train_epoch + 1}\")\n",
    "print(f\"训练准确率: {max_train_accuracy:.4f}\")\n",
    "print(f\"对应的测试准确率: {test_accuracy_at_max_train:.4f}\")\n",
    "print(f\"对应的测试损失: {test_loss_at_max_train:.4f}\")\n",
    "print(f\"对应的 Precision: {precision_at_max_train:.4f}\")\n",
    "print(f\"对应的 Recall: {recall_at_max_train:.4f}\")\n",
    "print(f\"对应的 F1-Score: {f1_at_max_train:.4f}\")\n",
    "print(\"==========================\\n\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chunjian_image",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
